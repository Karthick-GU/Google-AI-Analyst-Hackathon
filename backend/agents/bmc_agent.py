import io
import json
import base64, os
from os import environ
from io import BytesIO
from google import genai
from docx import Document
from google.genai import types
from google.cloud import storage
from google.cloud import storage
from google.adk.runners import Runner
# from PyPDF2 import PdfReader, PdfWriter
from google.oauth2 import service_account
from google.adk.sessions import InMemorySessionService
from google.adk.agents import LlmAgent, SequentialAgent

credentials = service_account.Credentials.from_service_account_info(
    {
        "type": "service_account",
        "project_id": environ["PROJECT_ID_SA"],
        "private_key_id": environ["PRIVATE_KEY_ID"],
        "private_key": environ["PRIVATE_KEY"].replace("\\n", "\n"),
        "client_email": environ["CLIENT_EMAIL"],
        "client_id": environ["CLIENT_ID"],
        "auth_uri": "https://accounts.google.com/o/oauth2/auth",
        "token_uri": "https://oauth2.googleapis.com/token",
        "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
        "client_x509_cert_url": environ["CLIENT_X509_CERT_URL"],
    }
)
 
 
def _is_local_path(p: str) -> bool:
    return os.path.exists(p)
 
 
def safe_load_json(s: str):
    """Safely parse JSON from LLM response, removing markdown wrappers."""
    s = s.strip()
    # remove triple-backtick wrappers if present
    s = s.replace("```json", "").replace("```", "").strip()
    return json.loads(s)
 
 
def _download_bytes_from_gcs_path(gcs_path: str) -> bytes:
 
    storage_client = storage.Client(
        project=environ["PROJECT_ID"],
        credentials=credentials,
    )
    # support gs://bucket/blob and plain blob (use BUCKET_NAME)
    if gcs_path.startswith("gs://"):
        _, _, rest = gcs_path.partition("gs://")
        bucket_name, _, blob_name = rest.partition("/")
        bucket = storage_client.bucket(bucket_name)
    else:
        bucket = storage_client.bucket(os.environ["GCS_BUCKET"])
        blob_name = gcs_path
    ## listing blobs for debug
    list_blobs = list(bucket.list_blobs())
 
    blob = bucket.blob(blob_name)
    data = BytesIO()
    blob.download_to_file(data)
    return data.getvalue()
 
def split_pdf_to_page_base64(pdf_bytes: bytes):
    """
    Yields (page_number, base64_page_pdf) for each page.
    Works for both text PDFs and scanned-image PDFs.
    """
    reader = PdfReader(io.BytesIO(pdf_bytes))
    num_pages = len(reader.pages)
    base64_pages = []
    for i in range(num_pages):
        writer = PdfWriter()
        writer.add_page(reader.pages[i])

        buf = io.BytesIO()
        writer.write(buf)
        page_bytes = buf.getvalue()

        page_b64 = base64.b64encode(page_bytes).decode("utf-8")
        base64_pages.append((i + 1, page_b64))
    return base64_pages
        
def read_docx(path: str) -> str:
    """Read and extract text from a DOCX file (local or GCS path)."""
    if _is_local_path(path):
        doc = Document(path)
    else:
        data = _download_bytes_from_gcs_path(path)
        doc = Document(BytesIO(data))
    return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
 
 
def read_pdf_as_base64(path: str) -> str:
    """Read a PDF file as base64 (local or GCS path)."""
    if _is_local_path(path):
        with open(path, "rb") as pdf_file:
            pdf_bytes = pdf_file.read()
    else:
        pdf_bytes = _download_bytes_from_gcs_path(path)

    return base64.b64encode(pdf_bytes).decode("utf-8")
 
 
def read_text_from_file(paths: str) -> str:
    """Accept a single path or comma-separated paths. Read each file
    (local or GCS) and concatenate their textual content.
 
    Returns a summary generated by the model for the combined content.
    """
    parts = [p.strip() for p in paths.split(",") if p.strip()]
    if not parts:
        raise ValueError("No paths provided to read_text_from_file")
 
    aggregated_texts = []
    for p in parts:
        ext = os.path.splitext(p)[1].lower()
        if ext == ".docx":
            text = read_docx(p)
            msg1_document1 = types.Part.from_text(text=text)
            aggregated_texts.append(msg1_document1)
        elif ext in [".txt", ".md"]:
            if _is_local_path(p):
                with open(p, "r", encoding="utf-8") as f:
                    aggregated_texts.append(f.read())
            else:
                data = _download_bytes_from_gcs_path(p)
                try:
                    msg1_document1 = types.Part.from_text(text=data.decode("utf-8"))
                    aggregated_texts.append(msg1_document1)
                except Exception:
                    aggregated_texts.append(data.decode("latin-1", errors="ignore"))
        elif ext == ".pdf":
            # include a short preview of the PDF (base64 truncated) so the model can use it
            b64 = read_pdf_as_base64(p)
            msg = types.Part.from_bytes(
                data=b64,
                mime_type="application/pdf",
            )
            # ## Adding page by page base64 content
            # for chunk in [b64[i : i + 5000] for i in range(0, len(b64), 5000)]:
            #     msg_chunk = types.Part.from_bytes(
            #         data=chunk,
            #         mime_type="application/pdf",
            #     )
            #     aggregated_texts.append(msg_chunk)

            aggregated_texts.append(msg)
        else:
            raise ValueError(f"Unsupported file type: {ext}")
    extracted_text = ""
    for text in aggregated_texts:
        extracted_text = extracted_text + extract_text(text) + " "
    # create a Part with the combined text and ask the model to summarise
    return extracted_text
 
 
def extract_text(content):
    client = genai.Client(
        vertexai=True,
        api_key=environ["VEXTEX_API_KEY"],
    )
 
    model = "gemini-3-pro-preview"
    contents = [
        types.Content(
            role="user",
            parts=[content, types.Part.from_text(text="""Extract all the points in concise manner.""")],
        ),
    ]
 
    generate_content_config = types.GenerateContentConfig(
        temperature=1,
        top_p=0.95,
        seed=0,
        max_output_tokens=65535,
        safety_settings=[
            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
            types.SafetySetting(
                category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"
            ),
            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF"),
        ],
        thinking_config=types.ThinkingConfig(
            thinking_budget=-1,
        ),
    )
    summarized_text = ""
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        summarized_text = summarized_text + chunk.text + " "
    return summarized_text
 
 
# Agent 1: Summary Generator
# The output will be placed in the SequentialAgent's result under the key 'summary_text'.
summary_agent = LlmAgent(
    name="SummaryAgent",
    model="gemini-2.0-flash",
    instruction="""
    You are a startup analyst. You will be provided with a document path.
    First, use the read_text_from_file tool to read the document content.
    Then summarize the document precisely.
 
    Output only a concise, professional summary paragraph.
    """,
    description="Summarizes startup-related content into key insights.",
    output_key="summary_text",
    tools=[read_text_from_file],
)
 
# Agent 2: Business Model Canvas (BMC) Generator
# The input for this agent comes from the output of the previous agent ({summary_text}).
bmc_agent = LlmAgent(
    name="BmcAgent",
    model="gemini-2.0-flash",
    instruction="""
        You are an expert business consultant.
        Analyze the project specification provided in the state as 'project_spec'.
 
        Create a comprehensive Business Model Canvas with these exact keys:
        - key-partners (2-4 items)
        - key-activities (2-4 items)
        - key-resources (2-4 items)
        - value-propositions (2-4 items)
        - customer-relationships (2-4 items)
        - channels (2-4 items)
        - customer-segments (2-4 items)
        - cost-structure (2-4 items)
        - revenue-streams (2-4 items)
 
        Each item should be a concise string (1-2 sentences max).
        Return ONLY valid JSON (no markdown, no commentary).
        """,
    description="Generates Business Model Canvas JSON from summary.",
    output_key="bmc_json",
)
 
 
# The sequential agent defines the overall flow: SummaryAgent runs first,
bmc_pipeline_agent = SequentialAgent(
    name="BmcPipelineAgent",
    sub_agents=[summary_agent, bmc_agent],
    description="Summarizes document and generates Business Model Canvas in JSON format.",
)
root_agent = bmc_pipeline_agent
 
 
APP_NAME = "ai_analyst"
USER_ID = "1234"
SESSION_ID = "session1234"
 
 
async def bmc_main(file_urls):
    session_service = InMemorySessionService()
    session = await session_service.create_session(
        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID
    )
    runner = Runner(
        agent=bmc_pipeline_agent, app_name=APP_NAME, session_service=session_service
    )
    raw = []
    async for event in runner.run_async(
        user_id=session.user_id,
        session_id=session.id,
        new_message=types.Content(
            role="user",
            parts=[types.Part(text=file_urls)],
        ),
    ):
        if (
            hasattr(event, "is_final_response")
            and event.is_final_response()
            and getattr(event, "content", None)
        ):
            part = [part.text for part in event.content.parts]
            raw.extend(part)
   
    parsed = safe_load_json(raw[-1])
 
    return parsed